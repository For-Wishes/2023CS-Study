# 💡 가상 메모리 관리

# ✅ 요구 페이징
프로세스가 필요로 하는 데이터를 언제 메모리로 가져올지 결정하는 것은 가져오기 정책이다.  
가져오기 정책은 프로세스가 요청할 때, 메모리로 가져오는 방법이 일반적인데 이를 요구 페이징(Demand Paging)이라고 한다.

<br/>

## 요구 페이징의 개요
컴퓨터를 오래 사용하다 보면 시스템이 느려진다. 이때 컴퓨터의 전원을 껐다가 다시 켜면 시스템이 빨라진다.  
컴퓨터를 오래 켜두면 시스템이 느려지는 이유는 작업을 하지 않고 쉬는 프로세스나 좀비 프로세스가 메모리를 차지하여 메모리 관리가 복잡해지기 때문이다.  
따라서 메모리에 꼭 필요한 프로세스만 유지하는 것이 좋다.

<br/>

용량이 큰 프로세스를 실행할 때, 운영체제는 프로세스를 구성하는 모듈을 전부 메모리에 올리지 않는다.  
필요한 모듈만 메모리에 올려 실행하고 나머지 모듈은 필요하다고 판단될 때 메모리로 불러온다.  
이렇게 **프로세스의 일부만 메모리로 가져오는 이유**는 다음과 같다.  
- 메모리를 효율적으로 관리하기 위해서이다.
    - 메모리가 꽉 차면 관리하기 어려우므로 가급적 적은 양의 프로세스만 유지한다.
- 응답 속도를 향상하기 위해서이다.
    - 용량이 큰 프로세스를 전부 메모리로 가져와 실행하면 응답이 늦어질 수 있으므로 필요한 모듈만 올려 실행한다.

<br/>

포토샵 같은 대형 프로그램에는 본 프로그램 외에도 피부 보정 필터, 노이즈 제거 필터 같은 외부 필터가 있다.  
그런데 포토샵을 실행할 때 본 프로그램과 외부 필터를 메모리에 모두 올리면 메모리를 많이 차지할 뿐 아니라 프로그램이 시작하는 시간도 오래 걸린다.  
따라서 메모리에는 포토샵의 본 프로그램만 올리고 필터는 사용자가 필요로 할 때마다 메모리로 가져오는 것이 효율적이다.

<br/>

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668598068896/RUi08QLbh.png?auto=compress,format&format=webp)

<br/>

포토샵과 같이 프로그램의 일부만 가져와 실행하고 사용자가 특정 기능을 요구할 때 해당 모듈을 메모리에 올리면 **메모리의 절약**, **메모리의 효율적 관리**, **프로세스의 응답 속도 향상** **등**의 효과를 볼 수 있다.  
이처럼 사용자가 요구할 때 해당 페이지를 메모리로 가져오는 것을 요구 페이징이라고 한다.

<br/>

**미리 가져오기**는 요구 페이징과 반대로 앞으로 필요할 것이라고 예상되는 페이지를 미리 가져오는 방식이다.  
미리 가져오기의 대표적인 경우가 캐시이다.  
캐시는 앞으로 필요할 것이라고 예상되는 부분을 고속의 캐시 메모리에 가져다 놓음으로써 시스템의 성능을 향상한다.  
그러나 미리 가져온 데이터가 쓸모없을 경우 피해가 매우 크다.  
따라서 현대의 운영체제는 요구 페이징을 기본으로 사용하고 있다.

<br/>

> 스와핑과 게으른 스와퍼  
> : 프로세스 입장에서는 프로세스를 구성하는 모든 페이지가 한꺼번에 메모리로 올라오는 것이 좋다. 이렇게 프로세스를 구성하는 모든 페이지를 메모리에 올리는 것을 순수한 스와핑(Swapping)이라고 한다. 이와 달리 사용자가 요구할 때 메모리에 올리는 것은 게으른 스와퍼(Lazy Swapper)라고 한다.
> 

<br/>

## 페이지 테이블 엔트리의 구조
가상 메모리의 크기는 물리 메모리와 스왑 영역을 합친 것이다.  
스왑 영역은 하드디스크에 존재하나 메모리 관리자가 관리하는 영역으로 가상 메모리의 구성 요소 중 하나이다.  
스왑 영역에서 물리 메모리로 데이터를 가져오는 것을 스왑인, 물리 메모리에서 스왑 영역으로 데이터를 내보내는 것을 스왑아웃이라고 한다.

<br/>

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668598643305/YtChym7Md.png?auto=compress,format&format=webp)

<br/>

가상 메모리 시스템에서 사용자의 프로세스는 물리 메모리와 스왑 영역 중 한 곳에 있다.  
이때 페이지가 스왑 영역에 있는 경우는 크게 두 가지이다.  
- 요구 페이징으로 인해 처음부터 물리 메모리에 올라가지 못한 경우
- 메모리가 꽉 차서 스왑 영역으로 옮겨 온 경우

어떤 경우든 페이지 테이블에는 페이지가 메모리에 있는지, 스왑 영역에 있는지 표시해야 하는데 이때 사용하는 비트가 유효 비트이다.

<br/>

아래 그림은 페이지 테이블 엔트리(PTE)의 구성을 나타낸 것이다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668599066882/DS_xgPbmN.png?auto=compress,format&format=webp)

<br/>

PTE는 페이지 테이블의 한 행을 말한다.  
PTE는 페이지 번호와 프레임 번호로 구성된다.  
정확히 말하면 **페이지 번호**, **플래그 비트**, **프레임 번호**로 구성된다. (몇몇 비트는 CPU에 따라 추가되거나 빠지기도 한다.)

<br/>

PTE의 맨 앞에 있는 페이지 번호는 주소 변환 방식 중 직접 매핑에서는 필요 없다.  
그러나 연관 매핑에서는 페이지 번호와 프레임 번호가 둘 다 필요하다.  
페이지 번호는 매핑 방식에 따라 포함되기도 하고 포함되지 않기도 하므로 위의 그림에는 흐리게 표시되어 있다.

<br/>

PTE의 마지막에 있는 프레임 번호는 가상 주소의 해당 페이지가 어느 프레임에 있는지 알려 주는 자료 구조로 페이지 테이블의 핵심이다.  
메모리 관리자는 찾은 프레임 번호를 이용하여 가상 주소를 물리 주소로 변환한다.  
프레임 번호는 주소 필드(Address Field)라고도 하므로 이후로는 주소 필드라고 칭하겠다.  
PTE의 가운데에는 접근 비트, 변경 비트, 유효 비트, 읽기 비트, 쓰기 비트, 실행 비트 등을 모아 놓은 플래그 비트가 있다.  
- 접근 비트(Access Bit)
    - 접근 비트는 페이지가 메모리에 올라온 후 사용한 적이 있는지 알려 주는 비트이다.
    - 해당 메모리에 읽기나 실행 작업을 했다면 접근 비트가 1이 된다.
    - 접근 비트는 참조 비트(Reference Bit)라고도 한다.
- 변경 비트(Modified Bit)
    - 변경 비트(Modified Bit)는 페이지가 메모리에 올라온 후 데이터의 변경이 있었는지 알려 주는 비트이다.
    - 해당 메모리에 쓰거나 추가 작업을 했다면 변경 비트가 1이 된다.
    - 변경 비트는 데이터가 새로운 값으로 오염되었다는 의미에서 더티 비트(Dirty Bit)라고도한다.
- 유효 비트(Valid Bit)
    - 유효 비트는 페이지가 실제 메모리에 있는지를 나타내는 비트이다.
    - 가상 메모리 시스템에서는 물리 메모리가 부족할 경우 일부 페이지가 스왑 영역으로 옮겨진다.
        - 이는 프로세스가 물리 메모리에 접근했을 때, 해당 데이터가 메모리에 있지 않고 저장 장치에 있을 수 있다는 의미이다.
    - 유효 비트는 해당 페이지가 메모리에 있는지를 나타내므로 현재 비트(Present Bit)라고도 한다.
- 읽기 비트(Read Bit), 쓰기 비트(Write Bit), 실행 비트(Execute Bit)
    - 읽기 비트, 쓰기 비트, 실행 비트는 페이지에 대한 읽기 권한, 쓰기 권한, 실행 권한을 나타내는 비트이다.
    - 읽기 권한이 없는 프로세스가 읽으려고 하거나 쓰기 권한이 없는 프로세스가 쓰려고 할 때, 접근을 차단하는 데 사용된다.
    - 읽기 비트, 쓰기 비트, 실행 비트를 합쳐서 접근 권한 비트(Rights Bit)라고도 부른다.
        - 세그먼테이션-페이징 혼용 기법에서 테이블의 크기를 줄이기 위해 접근 권한 비트를 세그먼테이션 테이블로 옮긴다는 것을 떠올려 보면 좋다.

<br/>

접근 비트와 변경 비트는 페이지가 메모리에 올라온 후, 어떤 작업이 있었는지 알려 주는 역할을 한다.  
두 비트는 메모리가 꽉 차서 어떤 페이지를 스왑 영역으로 옮겨야 할지 선택할 때 사용한다.

<br/>

## 페이지 부재
가상 메모리의 페이지 테이블에는 페이지가 물리 메모리에 있는지, 스왑 영역에 있는지 표시하기 위해 유효 비트를 사용한다.  
아래 그림은 유효 비트에 따른 주소 필드의 내용을 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668600989759/03bAUoMW5.png?auto=compress,format&format=webp)

<br/>

유효 비트가 0일 때는 페이지가 메모리에 있으므로 주소 필드에 물리 메모리의 프레임 번호가 저장된다.  
유효 비트가 1일 때는 페이지가 스왑 영역에 있으므로 주소 필드에 스왑 영역 내 페이지의 주소가 저장된다.  

<br/>

아래 그림은 가상 메모리, 페이지 테이블, 물리 메모리, 스왑 영역을 모두 나타낸 것이다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668601021254/ivZHVueC0.png?auto=compress,format&format=webp)

> [그림] 물리 메모리와 스왑 영역에 저장된 페이지 매칭
> 

<br/>

가상 메모리의 페이지 0은 물리 메모리의 프레임 3에 있기 때문에 PTE 0의 유효 비트는 0, 주소 필드 값은 프레임 번호 3이다.  
가상 메모리의 페이지 4는 스왑 영역의 1 번에 있기 때문에 PTE 4의 유효 비트는 1, 주소 필드 값은 스왑 주소 1이다.

<br/>

위의 그림에서 프로세스가 페이지 3을 요청했다고 가정한다.  
PTE 3의 유효 비트가 1, 주소 필드 값이 0이므로 페이지 3은 메모리에 없고 스왑 영역의 0 번에 있다.  
이렇게 프로세스가 페이지를 요청했을 때, 그 페이지가 메모리에 없는 상황을 페이지 부재(Page Fault)라고 한다.  
페이지 부재가 발생하면 프로세스가 해당 페이지를 사용할 수 있도록 스왑 영역에서 물리 메모리로 옮겨야 한다.

<br/>

아래 그림은 페이지 부재가 발생했을 때 메모리 관리자가 어떤 작업을 하는지를 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668601344277/AoSfhgQpp.png?auto=compress,format&format=webp)

> [그림] 페이지 부재 발생 시의 조치
> 

<br/>

프로세스가 페이지 3을 요청하면 페이지 테이블의 유효 비트가 1이기 때문에 페이지 부재가 발생한다.  
메모리 관리자는 스왑 영역의 0 번에 있는 페이지를 메모리의 비어 있는 프레임인 5로 가져온다. (스왑인)  
프레임 5에 페이지가 들어오면 PTE 3의 유효 비트는 1에서 0으로, 주소 필드 값은 0에서 5로 바뀐다.  
그리고 프레임 5로 접근하여 해당 데이터를 프로세스에 넘긴다.

<br/>

페이지 부재가 발생하면 위의 그림 같은 과정을 거쳐 스왑 영역에 있는 페이지를 메모리의 빈 영역에 올리고 페이지 테이블을 갱신(업데이트)한다.  
위의 그림에서는 메모리에 빈 프레임이 있어 작업이 수월하지만, 빈 프레임이 없을 때는 메모리에 있는 프레임 중 하나를 스왑 영역으로 내보낸 후에야 해당 페이지를 가져올 수 있다.  
어떤 페이지를 스왑 영역으로 내보낼지 결정하는 알고리즘을 페이지 교체 알고리즘(Page Replacement Algorithm)이라고 하며, 페이지 교체 알고리즘에 의해 스왑 영역으로 보낼 페이지를 대상 페이지(Victim Page)라고 한다.

<br/>

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668601779130/1iB_J-A19.png?auto=compress,format&format=webp)

> [그림] 대상 페이지의 스왑아웃
> 

<br/>

위의 그림은 메모리가 꽉 찬 상태에서 페이지 부재가 발생했을 때, 어떤 작업이 일어나는지를 보여 준다.  
이전 그림의 상황에 이어서 프로세스가 페이지 4를 요청했다고 가정한다.  
1. 해당 페이지의 유효 비트가 1이다. 즉, 페이지 부재가 발생한다.
2. 메모리가 꽉 차 있는 상태이기 때문에 스왑 영역에 있는 페이지 E를 가져오기 위해 메모리의 페이지 중 하나를 스왑 영역으로 내보내야 한다.
물리 메모리의 프레임 3에 저장된 페이지를 대상 페이지로 선정했는데 이 페이지를 스왑 영역으로 옮긴다. (스왑아웃)
3. 이에 따라 대상 페이지 PTE 1의 유효 비트가 0에서 1로, 주소 필드 값이 프레임 3에서 스왑 주소 6으로 바뀐다.
4. 스왑 영역 1 번에 있던 페이지 E가 프레임 3으로 올라간다. (스왑인)
5. 이에 따라 PTE 4의 유효 비트가 1에서 0으로, 주소 필드 값이 스왑 주소 1에서 프레임 3으로 바뀐다.

<br/>

세그먼테이션 오류와 페이지 부재는 많은 차이가 있다.  
- 세그먼테이션 오류는 사용자의 프로세스가 주어진 메모리 공간을 벗어나거나 접근 권한이 없는 곳에 접근할 때 발상핸다.
    - 즉, 사용자 프로세스에 의해 발생하며 해당 프로세스를 강제 종료하여 해결한다.
- 페이지 부재는 해당 페이지가 물리 메모리에 없을 때 발생하는 오류로 사용자 프로세스와 무관하다.
    - 페이지 부재가 발생하면 메모리 관리자는 스왑 영역에서 해당 페이지를 물리 메모리로 옮긴 후 작업을 진행한다.

<br/>

## 지역성(Locality)
메모리가 꽉 차서 어떤 페이지를 스왑 영역으로 보낼 때는 되도록 앞으로 사용하지 않을 페이지를 쫓아내는 것이 좋다.  
자주 사용될 페이지를 쫓아내면 다시 물리 메모리로 불러와야 하기 때문에 시스템의 성능이 떨어진다.  
페이지 교체 알고리즘이 쫓아낼 페이지를 찾을 때는 지역성을 바탕으로 한다. 

<br/>

지역성은 기억 장치에 접근하는 패턴이 메모리 전체에 고루 분포되는 것이 아니라 특정 영역에 집중되는 성질을 말한다.  
문헌에 따라 국부성 또는 집약성이라고도 하며 크게 공간의 지역성, 시간의 지역성, 순차적 지역성으로 나뉜다.

<br/>

아래 그림은 각 지역성의 특징을 나타냈다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668602373088/vMkuYjnE4.png?auto=compress,format&format=webp)

> 공간의 지역성, 시간의 지역성, 순차적 지역성
> 

<br/>

- 공간의 지역성(Spatial Locality)
    - 공간의 지역성은 현재 위치에서 가까운 데이터에 접근할 확률이 먼 거리에 있는 데이터에 접근할 확률보다 높다는 것이다.
    - ex : 집 바로 옆의 편의점과 길 건너편의 편의점 중 가까이 있는 편의점에 갈 확률이 더 높다.
- 시간의 지역성(Temporal Locality)
    - 시간의 지역성은 현재를 기준으로 가장 가까운 시간에 접근한 데이터가 더 먼 시간에 접근한 데이터보다 사용될 확률이 높다는 것이다.
    - ex : 어제 구매한 음악 CD와 1 년 전에 구매한 CD 중 어제 산 CD를 들을 확률이 더 높다.
- 순차적 지역성(Sequentail Locality)
    - 순차적 지역성은 여러 작업이 순서대로 진행되는 경향이 있다는 것을 의미한다.
    - 일반적인 프로그래밍은 처음부터 마지막 순서로 진행되는 경향이 있다.
    - 문헌에 따라서는 순차적 지역성을 공간의 지역성의 특별한 경우로 보고 지역성을 공간의 지역성과 시간의 지역성으로만 구분하기도 한다.

<br/>

지역성 이론은 많은 곳에서 사용된다.  
특히 캐시는 지역성 이론을 사용하는 대표적인 장치이다.  
- 캐시
    - 지역성 이론을 사용하는 대표적인 장치
    - 시간적으로나 지역적으로 가까이 있는 데이터를 가져옴으로써 캐시 적중률을 높인다.
    - ex : 현재 5 번 행을 실행하고 있다면 10~19 번 행과 200~219 번 행 중에서 당연히 10~19 번 행을 가져오는 것이 유리하다.
        - 프로그램을 작성할 때 goto 문을 사용하지 말라고 하는 것도 이러한 이유에서이다.
            - 지역성에 근거하여 현재 실행하는 행과 가까운 행을 캐시 메모리로 가져오고 있는데 갑자기 goto 문을 사용하여 엉뚱한 행으로 이동해버리면 이미 가져온 데이터가 쓸모없어진다.

<br/>

캐시 메모리의 동작과 마찬가지로 페이지 교체 알고리즘에서도 지역성을 고려하여 대상 페이지를 선정한다.  
자주 사용하는 페이지를 대상 페이지로 선정하면 시스템의 성능이 저하될 것이므로 페이지 교체 알고리즘에서는 앞으로 적게 사용될 페이지를 대상 페이지로 선정함으로써 페이지 부재를 줄이고 컴퓨터의 성능을 높인다.

<br/>
<br/>

# ✅ 페이지 교체 알고리즘
메모리가 꽉 찼을 때 어떤 페이지를 스왑 영역으로 내보낼지 결정하는 재배치 정책에 대해 알아본다.

<br/>

## 페이지 교체 알고리즘의 개요
프로세스가 요구한 페이지가 현재 메모리에 없으면 페이지 부재가 발생한다.  
페이지 부재가 발생하면 스왑 영역에서 페이지를 메모리로 가져온다.   
그런데 만약 메모리가 꽉 찼다면 메모리에 있는 페이지를 스왑 영역으로 내보내야 한다.

<br/>

페이지 교체 알고리즘은 스왑 영역으로 보낼 페이지를 결정하는 알고리즘으로, 메모리에서 앞으로 사용할 가능성이 적은 페이지를 대상 페이지로 선정하여 페이지 부재를 줄이고 시스템의 성능을 향상한다.

<br/>

### 페이지 교체 알고리즘의 종류
- 페이지 교체 알고리즘의 종류와 특징

| 종류 | 알고리즘 | 특징 |
| --- | --- | --- |
| 간단한 알고리즘 | 무작위 | 무작위로 대상 페이지를 선정하여 스왑 영역으로 보낸다. |
|  | FIFO | 처음 메모리에 올라온 페이지를 스왑 영역으로 보낸다. |
| 이론적 알고리즘 | 최적 | 미래의 접근 패턴을 보고 대상 페이지를 선정하여 스왑영역으로 보낸다.  |
| 최적 근접 알고리즘 | LRU | 시간적으로 멀리 떨어진 페이지를 스왑 영역으로 보낸다. |
|  | LFU | 사용 빈도가 적은 페이지를 스왑 영역으로 보낸다.  |
|  | NUR | 최근에 사용한 적이 없는 페이지를 스왑 영역으로 보낸다. |
|  | FIFO 변형 | FIFO 알고리즘을 변형하여 성능을 높인다. |

<br/>

무작위 페이지 교체 알고리즘은 무작위로 대상 페이지를 선정한다.  
FIFO 페이지 교체 알고리즘은 처음 메모리에 올라온 페이지를 대상 페이지로 선정한다.  
최적 페이지 교체 알고리즘은 미래의 메모리 접근 패턴을 살펴보고 대상 페이지를 선정하므로 성능이 가장 좋지만 사실상 구현이 불가능하다.  
LRU, LFU, NUR 페이지 교체 알고리즘은 최적 페이지 교체 알고리즘에 근접하는 성능을 보이는 알고리즘으로, 세 알고리즘을 합쳐서 최적 근접 알고리즘이라고도 한다.  
최적 근접 알고리즘 중에는 FIFO 알고리즘을 변형한 2 차 기회 페이지 교체 알고리즘과 시계 알고리즘도 있다.  

<br/>

### 페이지 교체 알고리즘의 성능 평가 기준
알고리즘의 성능을 비교하는 기준을 알아본다.  
어떤 알고리즘이 다른 알고리즘보다 성능이 좋은지 평가하는 데에는 다양한 비교 방법이 있다.  
- ex : 같은 메모리 접근 패턴을 사용하는 A 알고리즘과 B 알고리즘을 실행하여
    - 페이지 부재 횟수 세기
    - 페이지를 요청한 후 실제로 작업에 들어갈 때까지의 평균 대기 시간 측정
    - 전체 작업에 걸리는 시간 비교
    - 같은 메모리 접근 패턴을 사용하여 페에지 부재 횟수와 페이지 성공 횟수 비교
        - 이후 알아볼 페이지 알고리즘들을 이 기준으로 비교한다.

<br/>

페이지 교체 알고리즘은 성능뿐 아니라 유지 비용도 고려해야 한다.  
아무리 성능이 뛰어난 알고리즘이라도 계산이 많이 필요하거나 메모리를 많이 차지한다면 알고리즘이 아니다.

<br/>

이후 알아볼 모든 페이지 교체 알고리즘은 아래 그림과 같은 메모리 접근 순서를 사용하고 물리 메모리는 3 개의 프레임을 가졌다고 가정한다.  
상단의 숫자는 메모리의 접근 순서를 나타내며 페이지는 번호 대신 알파벳을 사용한다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668684504570/gVuU9a-kL.png?auto=compress,format&format=webp)

> 공통으로 사용할 메모리 접근 패턴
> 

<br/>

## 무작위 페이지 교체 알고리즘(Random Page Replacement Algorithm)
무작위 페이지 교체 알고리즘은 페이지 교체 알고리즘 중 *가장 간단하게 구현*할 수 있는 방식이다.  
*스왑 영역으로 쫓아낼 대상 페이지를 특별한 로직 없이 무작위로 선정*한다.  
대부분 프로세스의 메모리 접근 패턴을 보면 메모리의 인접한 영역에 저장되는 지역성을 가진다.  
그런데 무작위 알고리즘은 이러한 *지역성을 전혀 고려하지 않기 때문에 자주 사용하는 페이지가 대상 페이지로 선정되기도 한다.*  
따라서 알고리즘의 *성능이 좋지 않아 거의 사용되지 않는다.*

<br/>

## FIFO 페이지 교체 알고리즘(First In First Out Page Replacement Algorithm)
선입선출 페이지 교체 알고리즘이라고도 하는 FIFO 페이지 교체 알고리즘은 시간상으로 메모리에 가장 먼저 들어온 페이지를 대상 페이지로 선정하여 스왑 영역으로 쫓아낸다.  

<br/>

아래 그림은 FIFO 페이지 교체 알고리즘의 동작을 나타낸 것이다.  
페이지 부재가 일어난 경우(원하는 페이지가 메모리에 없는 경우)는 F(Fail)로, 그렇지 않은 경우(원하는 페이지가 메모리에 있는 경우)는 S(Success)로 표시했다.

<br/>

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668684799285/u1w9cRqc4.png?auto=compress,format&format=webp)

> FIFO 페이지 교체 알고리즘의 동작
> 

<br/>

FIFO 페이지 교체 알고리즘은 큐로 구현한다.  
메모리의 맨 위에 있는 페이지는 가장 오래된 페이지이고, 새로운 페이지는 항상 맨 아래에 삽입된다.  
메모리가 꽉 차면 맨 위의 페이지가 스왑 영역으로 가고 나머지 페이지들이 위쪽으로 이동하며, 새로운 페이지가 아래쪽의 남은 공간에 들어온다. 

<br/>

위의 그림의 메모리 접근 순서 4 번을 보면 가장 오래된 페이지 A가 스왑 영역으로 가고, 페이지 B와 C가 위쪽으로 이동하며, 새로운 페이지 D가 아래쪽의 남은 공간에 들어온다.  
5 번의 경우는 메모리에 있던 페이지 B를 요청했으므로 성공(S)이다.  
교체되어 새로 들어오는 페이지는 회색으로, 성공한 페이지는 초록색으로 구분하여 나타냈다.  
위의 그림에서는 총 10 번의 페이지 요구에 대해 3 번만 성공했다.  
즉, 페이지 부재가 7 번 발생했다.

<br/>

페이지 교체 알고리즘에서는 앞으로 사용하지 않을 페이지를 스왑 영역으로 옮기는 것이 중요하다.  
FIFO 페이지 교체 알고리즘은 큐로 쉽게 구현할 수 있지만, 먼저 들어온 페이지를 항상 스왑 영역으로 옮긴다.  
시간의 지역성을 고려하면 가장 오래된 페이지를 대상 페이지로 선정하는 것이 맞다.  
그러나 메모리에 올라온 지 오래되었더라도 자주 사용되는 페이지가 있는데, FIFO 페이지 교체 알고리즘에서는 메모리에 올라온 시간만 고려하기 때문에 자주 사용되는 페이지가 스왑 영역으로 옮겨지기도 한다.

> 위의 그림의 메모리 접근 순서 5~7 번을 보면 5 번에서 사용되었던 페이지 B가 6 번에서 스왑 영역으로 옮겨졌다가 7 번에서 다시 메모리로 올라온다.
> 

<br/>

메모리에 먼저 올라왔어도 자주 사용되는 페이지가 있기도 하고, 나중에 올라왔어도 한 번만 사용되는 페이지가 있기도 하다.  
FIFO 페이지 교체 알고리즘은 무조건 오래된 페이지를 대상 페이지로 선정하기 때문에 성능이 떨어지는데 이러한 문제점을 개선한 것이 2 차 기회 페이지 교체 알고리즘이다.

<br/>

## 최적 페이지 교체 알고리즘(Optimal Page Replacement Algorithm)
최적 페이지 교체 알고리즘은 앞으로 사용하지 않을 페이지를 스왑 영역으로 옮긴다.  
메모리가 앞으로 사용할 페이지를 미리 살펴보고 페이지 교체 선정 시점부터 사용 시점까지 가장 멀리 있는 페이지를 대상 페이지로 선정한다.

<br/>

아래 그림은 최적 페이지 교체 알고리즘의 동작을 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668685659853/2vCUc6Xgv.png?auto=compress,format&format=webp)

> 최적 페이지 교체 알고리즘의 동작
> 

<br/>

FIFO 페이지 교체 알고리즘에서는 페이지 A가 스왑 영역으로 쫓겨나고 페이지 D가 들어왔다.  
그러나 최적 페이지 교체 알고리즘에서는 앞으로 사용할 페이지에 A, B, C가 있는지 살펴본다.  
그 결과 페이지 A는 6 번에서, 페이지 B는 5 번에서, 페이지 C는 9 번에서 사용되므로 가장 늦게 사용되는 페이지 C를 스왑 영역으로 보낸다.

<br/>

최적 페이지 교체 알고리즘은 미래의 메모리 접근 패턴을 보고 대상 페이지를 결정하기 때문에 성능이 좋다.  
하지만 미래의 접근 패턴을 안다는 것이 불가능하여 실제로 구현할 수 없다.  
이상적인 방법이지만 실제로 구현할 수 없으므로 구현이 가능하면서도 성능이 최적 페이지 교체 알고리즘에 근접하는 방법을 연구한 결과 최근 최소 사용 알고리즘인 LRU(Least Recently Used), 최소 빈도 사용 알고리즘인 LFU(Least Frequently Used), 최근 미사용 알고리즘인 NUR(Not Used Recently) 등이 개발되었다.  
이러한 알고리즘은 과거의 데이터를 바탕으로 미래의 접근 패턴을 추정하기 때문에 최적 근접 알고리즘(Optimal Approximation Algorithm)이라고 부른다.

<br/>

아래의 그림은 대표적인 최적 근접 알고리즘인 LRU 페이지 교체 알고리즘과 LFU 페이지 교체 알고리즘의 차이를 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668685778712/uVFhbpcd7.png?auto=compress,format&format=webp)

<br/>

- LRU 페이지 교체 알고리즘
    - 페이지에 접근한 시간을 기준으로 대상 페이지를 선정한다.
    - 현재 시간이 11 시이고 페이지 A, B, C에 마지막으로 접근한 시간이 각각 1 시 40 분, 8 시 50 분, 9 시 30 분이라면 현재와의 시간 차가 가장 큰 1 시 40 분에 접근한 페이지 A가 스왑 영역으로 옮겨진다.
- LFU 페이지 교체 알고리즘
    - 페이지가 사용된 횟수를 기준으로 대상 페이지를 선정한다.
    - 페이지 A, B, C가 메모리에 올라온 후 각각 1040 번, 8 번, 430 번 사용되었다면 8 번 사용한 페이지 B를 스왑 영역으로 옮긴다.

<br/>

최적 페이지 교체 알고리즘은 미래의 데이터를 참고로 하기 때문에 구현이 불가능한 반면, 최적 근접 알고리즘은 과거의 데이터로부터 미래를 예측하기 때문에 최적 페이지 교체 알고리즘과 유사한 성능을 보인다.

<br/>

## LRU 페이지 교체 알고리즘(Least Recently Used Page Replacement Algorithm)
최적 근접 알고리즘 중 하나로 ‘최근 최소 사용 페이지 교체 알고리즘’ 이라고도 한다.  
즉, 최근에 사용된 페이지는 놔두고 오래전에 사용된 페이지를 대상 페이지로 선정한다.  
LRU 페이지 교체 알고리즘은 시간을 기준으로 구현할 수 있으며 카운터나 참조 비트를 이용하는 방법도 있다. 

<br/>

### 페이지 접근 시간에 기반한 구현
LRU 페이지 교체 알고리즘의 가장 간단한 형태는 페이지에 접근한 시간을 기록하여 구현하는 것이다.

<br/>

아래의 그림은 LRU 페이지 교체 알고리즘의 동작을 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668848473472/sKymZnCLL.png?auto=compress,format&format=webp)

<br/>

FIFO 페이지 교체 알고리즘이 메모리에 올라온 시간을 기준으로 가장 오래된 페이지를 교체한다면, LRU 페이지 교체 알고리즘은 페이지에 접근한 지 가장 오래된 페이지를 교체한다.  
즉, 페이지에 읽기, 쓰기, 실행과 같은 연산이 이루어진 시간을 기준으로 하는 것이다.

<br/>

위의 그림에서는 편의상 맨 위의 숫자를 초 단위로 가정하고, 페이지가 메모리에 올라오거나 사용될 때마다 그 시간(초)을 사각형 아래에 표시했다.  
예를 들면 3 초에 페이지 C가 메모리에 올라왔으므로 C 아래의 숫자는 3이다.  
5 초에는 페이지 B에 접근했으므로 B 아래의 2가 5로 변경된다.  
또한 8 초에도 페이지 A에 접근했으므로 A 아래의 6이 8로 변경된다.

<br/>

LRU 페이지 교체 알고리즘에서는 숫자가 가장 작은 페이지 즉, 사용된 지 가장 오래된 페이지를 대상 페이지로 선정한다.  
예를 들자면, 6 초의 경우 페이지 A를 올리기 위해 가장 오랫동안 접근하지 않았던 페이지 C를 스왑 영역으로 옮긴다.  
9 초의 경우도 가장 오랫동안 접근하지 않았던 페이지 D를 스왑 영역으로 옮기고 그 자리에 페이지 C를 옮긴다.

<br/>

같은 메모리 접근 패턴에 대해 FIFO 페이지 교체 알고리즘의 페이지 성공 횟수는 3이고 LRU 페이지 교체 알고리즘의 페이지 성공 횟수는 4이다.  
주의할 점은 메모리 접근 패턴을 변경하면 LRU 페이지 교체 알고리즘의 성능이 FIFO 페이지 교체 알고리즘만큼 느려지기도 하고 최적 페이지 교체 알고리즘만큼 좋아지기도 한다는 사실이다.  
일반적으로 *LRU 페이지 교체 알고리즘의 성능은 FIFO 페이지 교체 알고리즘보다 우수하고 최적 페이지 교체 알고리즘보다는 조금 떨어지는 것으로 알려져 있다.*

<br/>

### 카운터에 기반한 구현
LRU 페이지 교체 알고리즘은 페이지 접근 시간을 기록하여 구현할 수도 있지만, 카운터를 사용하여 구현할 수도 있다.  
위의 LRU 페이지 교체 알고리즘의 동작 그림에서 맨 위의 숫자를 시간(초)이 아니라 카운터 숫자라고 생각하면 된다.

<br/>

접근 시간을 기록하든 카운트를 하든 두 방법은 모두 추가적인 메모리 공간을 필요로 하는 것이 단점이다.  
예를 들어 0~1024 초를 표시하려면 10bit가 필요하고 더 큰 숫자를 표시하려면 더 많은 비트를 사용해야 한다.  
이러한 *추가 공간으로 인해 사용자가 사용할 수 있는 공간이 낭비된다.*

<br/>

### 참조 비트 시프트(Reference Bit Shift) 방식
LRU 페이지 교체 알고리즘을 실제로 구현하는 데 참조 비트 시프트 방식을 사용할 수도 있다.  
참조 비트 시프트 방식은 각 페이지에 일정 크기의 참조 비트를 만들어 사용하는 것이다.  
참조 비트의 초깃값은 0이며 페이지에 접근할 때마다 1로 바뀐다.  
또한 참조 비트는 주기적으로 오른쪽으로 한 칸씩 이동한다. (Shift)

<br/>

아래의 그림은 참조 비트 시프트 방식의 동작을 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668849613661/CfVDoAs0c.png?auto=compress,format&format=webp)

<br/>

위의 그림의 (a)에서는 페이지 B에 접근하면 페이지 B의 참조 비트 맨 앞(왼쪽)이 1이 된다..  
다음으로 페이지 A에 접근하면 모든 참조 비트가 오른쪽으로 한 칸 이동하고 페이지 A의 맨 앞 참조 비트가 1이 된다.  
그리고 페이지 B에 다시 접근하면 모든 참조 비트가 오른쪽으로 한 칸 이동하고 페이지 B의 맨 앞 참조 비트가 1이 된다.  
이와 같은 방식으로 참조 비트를 갱신하다가 대상 페이지를 선정할 필요가 있으면 참조 비트 중 가장 작은 값을 대상 페이지로 선정한다.  
위의 그림에서는 페이지 C가 대상 페이지로 선정된다.

<br/>

참조 비트 시프트 방식은 LFU 페이지 교체 알고리즘과 혼동되기도 한다.  
LFU 페이지 교체 알고리즘은 페이지의 접근 횟수를 측정하여 대상 페이지를 선정하는데, 아래의 그림을 보면 참조 비트 시프트 방식이 LRU 페이지 교체 알고리즘이라는 것을 이해할 수 있을 것이다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668849633697/ttq8ymndt.png?auto=compress,format&format=webp)

<br/>

위의 그림에서는 페이지 A에 4 번, 페이지 B에 1 번, 페이지 C에 5 번 접근했다.  
그러나 참조 비트 중 가장 큰 값은 가장 최근에 접근한 페이지 B이고, 가장 작은 값은 가장 오랫동안 접근하지 않은 페이지 C이다.  
따라서 페이지 C가 대상 페이지로 선정된다.  
그림에서 보듯이 참조 비트 시프트 방식은 참조한 횟수가 아니라 참조한 시간을 기준으로 대상 페이지를 선정하므로 LRU  페이지 교체 알고리즘의 한 방식으로 분류된다.

<br/>

비록 1B이기는 하지만 참조 비트 시프트 방식도 작지 않은 공간을 사용하므로 공간을 낭비하는 것이 단점이다.  
LRU 페이지 교체 알고리즘의 단점은 접근 시간이나 참조 비트를 유지하기 위한 메모리가 추가로 필요하기 때문에 낭비되는 메모리 공간이 많다는 것이다.

<br/>

## LFU 페이지 교체 알고리즘(Least Frequently Used Page Replacement Algorithm)
LFU 페이지 교체 알고리즘은 ‘최소 빈도 사용 알고리즘’ 이라고도 한다.  
LFU 페이지 교체 알고리즘은 페이지가 몇 번 사용되었는지를 기준으로 대상 페이지를 선정한다.  
다시 말해 현재 프레임에 있는 페이지마다 그 동안 사용된 횟수를 세어 횟수가 가장 적은 페이지를 스왑 영역으로 옮긴다.

<br/>

아래의 그림은 LFU 페이지 교체 알고리즘의 동작을 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668849792225/lOYD8evEr.png?auto=compress,format&format=webp)

알파벳 아래의 숫자는 사용 빈도를 나타낸다.  
예를 들어, 5 번의 B(2)는 페이지 B가 메모리에 올라온 후 지금까지 2 번 사용되었다는 의미이다.  
위의 그림에서는 사용 빈도가 같은 경우 맨 위의 페이지를 대상 페이지로 선정했다.

<br/>

LFU 페이지 교체 알고리즘에서는 처음 메모리에 올라온 페이지의 사용 빈도가 1이고, 페이지가 사용될 때마다 하나씩 증가한다.  
위의 그림의 메모리 접근 순서 5 번을 보면 페이지 B의 사용 빈도가 1에서 2로 증가했다.  
이어서 메모리 접근 순서 6 번에서 페이지 부재가 일어나는데, 페이지 B는 대상 페이지에서 제외되고 사용 빈도가 같은 페이지 C와 D 중 맨 처음에 있는 페이지 D가 스왑 영역으로 쫓겨난다.

<br/>

같은 메모리 접근 패턴에 대해 LRU 페이지 교체 알고리즘의 페이지 성공 횟수는 4이고 LFU 페이지 교체 알고리즘의 페이지 성공 횟수는 5이다.  
LFU 페이지 교체 알고리즘이 LRU 페이지 교체 알고리즘보다 한 번 더 성공했지만, 일반적인 경우 두 알고리즘의 성능이 비슷하다고 알려져 있다.  
LRU, LFU 페이지 교체 알고리즘은 둘 다 FIFO 페이지 교체 알고리즘보다 성능이 우수하다.

<br/>

LFU 페이지 교체 알고리즘의 단점은 LRU 페이지 교체 알고리즘과 마찬가지로 낭비되는 메모리 공간이 많다는 것이다.  
페이지 접근 횟수(빈도)를 표시하는 데 추가 공간이 필요하므로 그만큼 메모리가 낭비된다.

<br/>

## NUR 페이지 교체 알고리즘(Not Used Recently Page Replacement Algorithm)
NUR 페이지 교체 알고리즘은 LRU, LFU 페이지 교체 알고리즘과 성능이 거의 비슷하면서도 불필요한 공간 낭비 문제를 해결한 알고리즘이다.  
NUR 페이지 교체 알고리즘은 ‘최근 미사용 페이지 교체 알고리즘’ 이라고도 불린다.

<br/>

알고리즘에서 접근 시간이든 접근 빈도든 정확한 값을 유지하는 것은 공간만 많이 차지할 뿐 의미가 없다.  
NUR 페이지 교체 알고리즘은 이러한 경향을 반영한 방식으로, 추가 비트 2 개 만 사용하여 미래를 추정한다.

<br/>

NUR 페이지 교체 알고리즘에서는 페이지마다 참조 비트와 변경 비트를 가지므로 페이지마다 추가되는 메모리 공간이 2 비트뿐이다.  
여기서 참조 비트는 앞서 살펴보았던 PTE의 접근 비트를 가리키고, 변경 비트는 PTE의 변경 비트를 가리킨다.  
참조 비트와 변경 비트는 초깃값이 0이며 다음과 같은 경우에 1이 된다.  
- 참조 비트    
    : 페이지에 접근(Read/Execute)하면 1이 된다.    
- 변경 비트    
    : 페이지가 변경(Write/Append)되면 1이 된다.
    
<br/>

모든 페이지의 초기 상태는 (0, 0)이다.  
이 상태에서 페이지에 읽기 또는 실행 같은 ‘접근’ 이 발생하면 (1, 0)으로 바뀐다.  
만약 페이지에 쓰기 또는 추가 같은 ‘변경’ 이 일어나면 (0, 1)이 된다.  
또한 접근과 변경, 2 가지 연산이 다 발생하면 (1, 1)이 된다.

<br/>

NUR 페이지 교체 알고리즘에서 대상 페이지를 선정할 때는 (0, 0), (0, 1), (1, 0), (1, 1_ 중에 하나를 고르는데, 가장 먼저 (0, 0)인 페이지를 선정한다.  
즉, 접근한 적도 변경한 적도 없는 페이지를 스왑 영역으로 옮긴다.  
만약 (0, 0)인 페이지가 없다면 (0, 1)인 페이지를, (0, 1)인 페이지가 없다면 (1, 0)인 페이지를, (1, 0)인 페이지도 없다면 최종적으로 (1, 1_인 페이지를 스왑 영역으로 옮긴다.

<br/>

아래 그림은 NUR 페이지 교체 알고리즘에서 대상 페이지를 선정하는 순서를 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668850987557/79XLCESwW.png?auto=compress,format&format=webp)

<br/>

NUR 페이지 교체 알고리즘에서 우선 고려 대상은 참조 비트이다.  
참조 비트가 0인 페이지를 먼저 찾고, 없으면 변경 비트가 0인 페이지를 찾는다.  
만약 같은 비트의 페이지가 여러 개라면 무작위로 대상 페이지를 선정한다.  
흔한 경우는 아니지만 모든 페이지의 비트가 (1, 1)일 때는 어떤 페이지가 더 자주 사용되는지 알 수 없어 NUR 페이지 교체 알고리즘을 정상적으로 적용할 수 없다.  
그러므로 NUR 페이지 교체 알고리즘에서 *모든 페이지가 (1, 1)이 되면 모든 페이지 비트를 (0, 0)으로 초기화한다. (Reset)*

<br/>

아래 그림은 NUR 페이지 교체 알고리즘의 동작을 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668851068242/9H9vgKH25.png?auto=compress,format&format=webp)

<br/>

알파벳 아래의 첫 번째 숫자는 참조 비트, 두 번째 숫자는 변경 비트를 나타낸다.  
메모리 접근 패턴 그림에서 읽기와 쓰기를 구분하지 않았기 대문에 모든 작업은 읽기 연산이라고 가정한다.  
그러므로 위의 그림에서 변경 비트가 1이 되는 경우는 없다.  
또한 (0, 0)인 페이지가 여러 개일 때는 가장 위에 있는 페이지를 대상 페이지로 선정했다.

<br/>

위의 그림에서 처음 메모리에 올라온 모든 페이지의 참조 비트와 변경 비트는 (0, 0)이다.  
이후 메모리 접근 순서 5 번에서 페이지 B에 접근하면 페이지 B의 비트가 (1, 0)으로 바뀐다.  
다음으로 메모리 접근 순서 6 번에서 페이지 A에 접근하면 페이지 B는 대상 페이지에서 제외되고 페이지 C와 D 중 가장 위에 있는 페이지 D가 스왑 영역으로 쫓겨난다.  
최종적으로 페이지 성공 횟수는 5이다.  
이는 LFU 페이지 교체 알고리즘의 페이지 성공 횟수와 같은데 단 2bit만 추가하여 LFU 페이지 교체 알고리즘과 비슷한 성능을 보인 것이다.

<br/>

최적 근접 알고리즘인 LRU, LFU, NUR 페이지 교체 알고리즘의 성능은 거의 비슷하며 FIFO 페이지 교체 알고리즘보다 우수하다.  
이 중에서 NUR 페이지 교체 알고리즘은 2bit만 추가하여 다른 알고리즘과 유사한 성능을 낼 뿐만 아니라 쉽게 구현할 수 있다는 장점 때문에 가장 많이 사용되고 있다.

<br/>

## FIFO 변형 알고리즘
FIFO 페이지 교체 알고리즘은 메모리에 올라온 순서만 고려하고 자주 사용하는 페이지를 고려하지 않기 때문에 성능이 좋지 않다.  
이러한 단점을 개선한 알고리즘으로 2차 기회 페이지 교체 알고리즘과 시계 알고리즘이 있다.  
이 두 알고리즘은 FIFO 페이지 교체 알고리즘의 방식을 기본으로 하되 페이지에 접근할 때마다 순서의 변화를 주어 성능을 향상한다.

<br/>

### 2 차 기회 페이지 교체 알고리즘(Second Chance Page Replacement Algorithm)
2 차 기회 페이지 교체 알고리즘은 FIFO 페이지 교체 알고리즘과 마찬가지로 큐를 사용하지만, 차이점은 특정 페이지에 접근하여 페이지 부재 없이 성공할 경우 해당 페이지를 큐의 맨 뒤로 이동하여 대상 페이지에서 제외한다는 것이다.  
다시 말해 성공한 페이지를 큐의 맨 뒤로 옮김으로써 기회를 한 번 더 준다.

<br/>

아래의 그림은 2 차 기회 페이지 교체 알고리즘의 동작을 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668853145792/KuOVlilMb.png?auto=compress,format&format=webp)

<br/>

FIFO 페이지 교체 알고리즘과 비교하여 메모리 접근 순서 5 번에서 맨 앞에 있던 페이지 B는 성공한 후 큐의 맨 뒤로 옮겨진다.  
FIFO 페이지 교체 알고리즘에서는 큐의 맨 뒤로 옮겨지면 대상 페이지로 선정될 확률이 줄어든다.  
2 차 기회 페이지 교체 알고리즘의 페이지 성공 횟수는 FIFO 페이지 교체 알고리즘보다 하나 더 많은 4이다.

<br/>

일반적으로 2 차 기회 페이지 교체 알고리즘의 성능은 LRU, LFU, NUR 페이지 교체 알고리즘보다 약간 낮고 FIFO 페이지 교체 알고리즘보다 약간 높은 것으로 알려져 있다.  
그러나 큐를 유지하는 비용이 높고, 페이지가 성공하면 큐의 중간에 있는 값을 뒤로 이동하는 작업이 추가되는 것이 단점이다.

<br/>

> 2 차 기회 페이지 교체 알고리즘은 FIFO 페이지 교체 알고리즘을 변형한 것이기 때문에 2 차 기회 FIFO 페이지 교체 알고리즘(Second Chance FIFO Page Replacement Algorithm)이라고도 한다.
> 

<br/>

### 시계 알고리즘(Clock Algorithm)
시계 알고리즘은 2 차 기회 페이지 교체 알고리즘과 유사하여 두 알고리즘을 같은 알고리즘으로 보기도 하지만 실제 구현은 서로 다르다.

<br/>

아래 그림은 시계 알고리즘의 특징을 보여 준다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668853336319/wr_0o3Z93.png?auto=compress,format&format=webp)

2 차 기회 페이지 교체 알고리즘은 큐를 사용하지만, 시계 알고리즘은 원형 큐를 사용하는 것이 가장 큰 차이점이다.

<br/>

시계 알고리즘에서는 스왑 영역으로 옮길 대상 페이지를 가리키는 포인터를 사용하는데, 이 포인터가 큐의 맨 바닥으로 내려가면 다음 번에는 다시 큐의 처음을 가리키게 된다.  
위의 그림에서 보듯이 포인터가 시계처럼 한 방향으로 돌기 때문에 시계 알고리즘이라고 부르는 것이다.

<br/>

아래 그림은 시계 알고리즘의 동작을 보여 준다.

![Untitled](https://yansigit.github.io/posts/%ED%8E%98%EC%9D%B4%EC%A7%80-%EA%B5%90%EC%B2%B4-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/Untitled%2012.png)

<br/>

2 차 기회 페이지 교체 알고리즘에 비해 각 페이지에 참조 비트가 하나씩 추가된다.  
참조 비트의 초깃값은 0이며 메모리에 있는 페이지를 성공적으로 참조하면 0에서 1로 변경된다.  
시계 알고리즘의 대상 포인터는 메모리가 꽉 찰 경우 스왑 영역으로 쫓겨날 페이지를 가리킨다.  
만약 *가리키는 페이지가 스왑 영역으로 쫓겨나면 대상 포인터를 밑으로 이동한다.*  
이때 참조 비트가 1인 페이지는 건너뛰고, 메모리의 바닥에 도착하면 원형 큐처럼 다시 메모리의 상단으로 이동한다.  
이렇게 참조 비트가 1인 페이지를 대상 페이지에서 제외하는 이유는 2 차 기회 페이지 교체 알고리즘처럼 *기회를 한 번 더 주기* 위해서이다.  
그러나 대상에서 제외되는 경우는 단 한 번뿐이고, *참조 비트가 1인 페이지를 건너뛸 때는 0으로 바꿔 놓는다.*  
이는 한 바퀴를 돌아 다시 대상 포인터가 오면 제외하지 않겠다는 의미이다.

<br/>

위의 그림의 동작을 메모리 접근 순서별로 살펴보면 다음과 같다.  
1. 1 번에서 페이지 A가 메모리에 올라오면 대상 포인터는 페이지 A를 가리킨다.
2. 2, 3 번에서 페이지 B와 C가 메모리에 올라온다.
3. 4 번에서 대상 포인터가 가리키는 페이지 A가 스왑 영역으로 가고 페이지 D가 메모리에 올라온다.
대상 포인터는 한 칸 아래인 페이지 B로 이동한다.
4. 5 번에서 페이지 B가 페이지 참조에 성공하면 페이지 B의 참조 비트는 1이 되어 기회를 얻는다.
대상 포인터는 페이지 B의 참조 비트를 0으로 만든 후 한 칸 아래인 페이지 C로 이동한다.
5. 6 번에서 대상 포인터가 가리키는 페이지 C가 스왑 영역으로 가고 페이지 A가 메모리에 올라온다.
대상 포인터는 원을 돌아 메모리의 맨 위인 페이지 D로 이동한다.
6. 7, 8 번에서 페이지 B와 A를 성공적으로 참조하여 참조 비트를 1로 바꾼다.
7. 9 번에서 대상 포인터가 가리키는 페이지 D가 스왑 영역으로 가고 페이지 C가 메모리에 올라온다.
이때 대상 포인터는 밑으로 이동하는데, 페이지 B와 A 둘 다 참조 비트가 1이므로 0으로 변경한 후 건너뛴다.
대상 포인터는 메모리를 돌아서 맨 위의 페이지 C를 가리킨다. 

<br/>

시계 알고리즘은 대상 포인터와 각 페이지당 참조 비트 하나만 추가하면 되기 때문에 NUR 페이지 교체 알고리즘보다 추가 공간이 적게 들지만 알고리즘이 복잡하고 계산량이 많다는 것이 단점이다. 

<br/>
<br/>

# ✅ 스레싱과 프레임 할당
물리 메모리에는 여러 개의 프로세스가 올라와 있다.  
운영체제는 물리 메모리의 공간이 충분하면 프로세스의 요청에 따라 원하는 프레임을 할당하지만, 그렇지 못할 경우 남아 있는 프레임을 어떻게 나누어 주느냐는 문제에 맞닥뜨린다.

<br/>

## 스레싱
메모리가 꽉 찬 후에는 새로운 프로그램을 메모리에 올리기 위해 기존 프로그램을 스왑 영역으로 옮기는 횟수가 잦아지기 때문에 하드디스크와의 입출력이 계속되어 프로그램이 정지한 것 같은 현상이 발생할 수 있다.   
이와 같이 하드디스크의 입출력이 너무 많아져서 잦은 페이지 부재로 작업이 멈춘 것 같은 상태를 스레싱(Threshing)이라고 한다.

<br/>

### 물리 메모리의 크기와 스레싱
스레싱은 메모리의 크기가 일정할 경우 멀티 프로그램의 수와 밀접한 관계가 있다.  
동시에 실행하는 프로그램의 수를 멀티프로그래밍 정도(Degree Of Multiprogramming)라고 한다.  
멀티프로그래밍 정도가 너무 높으면 스레싱이 발생한다.

<br/>

아래의 그림은 멀티 프로그래밍 정도와 CPU 사용률의 관계를 나타낸 것이다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668853836857/mKrx0pmjv.png?auto=compress,format&format=webp)

> 멀티프로그래밍 정도와 스레싱
> 

<br/>

프로그램의 수가 적을 때는 CPU 사용률이 계속 증가하다가 메모리가 꽉 차면 CPU가 작업하는 시간보다 스왑 영역으로 페이지를 보내고 새로운 페이지를 메모리에 가져오는 작업이 빈번해져서 CPU가 작업할 수 없는 상태에 이르게 된다.  
이러한 시점을 스레싱 발생 지점(Threshing Point)이라고 한다. 

<br/>

물리 메모리를 늘리면 컴퓨터가 빨라진다.  
컴퓨터는 운영체제를 포함하여 많은 프로그램을 동시에 실행한다.  
자주 사용하는 프로세스가 필요로 하는 메모리보다 물리 메모리가 작다면 스레싱 발생 지점에 빨리 도달하여 컴퓨터가 전체적으로 느려진다.  
따라서 물리 메모리의 크기를 늘리면 스레싱 발생 지점이 늦춰져서 프로세스를 원만하게 실행할 수 있다. 

<br/>

물리 메모리의 용량을 512MB에서 4GB로 늘리면 스레싱 발생 지점이 늦춰지기 때문에 컴퓨터의 성능이 향상된다.  
그런데 물리 메모리의 용량을 4GB에서 16GB로 늘리면 대부분의 경우 빨라지지 않는다.  
물리 메모리가 작업을 하는 데 충분한 크기가 되면 그 이후에는 크기를 늘려도 작업 속도에 영향을 미치지 않는다.

<br/>

### 스레싱과 프레임 할당
스레싱은 각 프로세스에 프레임을 할당하는 문제와도 연관된다.  
실행 중인 여러 프로세스에 프레임을 얼마나 나누어 주느냐에 따라 시스템의 성능이 달라진다.  
어떤 프로세스에는 너무 적은 프레임을 할당하여 페이지 부재가 빈번히 일어나고, 어떤 프로세스에는 너무 많은 프레임을 할당하여 페이지 부재를 줄이는 대신 메모리를 낭비한다면 전반적으로 시스템의 성능이 낮아진다.  
따라서 남아 있는 프레임을 실행 중인 프로세스에 적절히 나누어 주는 정책이 필요하다.  
프로세스에 프레임을 할당하는 방식은 크게 정적 할당과 동적 할당으로 구분된다.  

<br/>

## 정적 할당(Static Allocation)
정적 할당 방식은 프로세스 실행 초기에 프레임을 나누어 준 후 그 코기를 고정하는 것으로, 균등 할당 방식과 비례 할당 방식이 있다.

<br/>

### 균등 할당(Equal Allocation)
균등 할당 방식은 프로세스의 크기와 상관없이 사용 가능한 프레임을 모든 프로세스에 동일하게 할당한다.

<br/>

아래의 그림은 균등 할당 방식을 나타낸 그림이다. 

![Untitled](https://shacoding.com/wp-content/uploads/2022/06/image-55.png)

<br/>

위의 그림에서는 현재 사용 가능한 프레임이 12 개이고 프로세스 A는 프레임 6 개를, 프로세스 B는 프레임 3 개를, 프로세스 C는 프레임 9 개를 필요로 한다.  
균등 할당 방식에서는 12 개의 가용 프레임을 3 등분 하여 각 프로세스에 4 개씩 나누어 준다.  

<br/>

균등 할당 방식에서는 크기가 큰 프로세스의 경우 필요한 만큼 프레임을 할당받지 못하기 때문에 페이지 부재가 빈번하게 발생하고, 크기가 작은 프로세스의 경우 메모리가 낭비된다.  
위의 그림에서는 프로세스 B에 프레임 3 개만 있으면 되는데 4 개가 할당되었고, 프로세스 C는 프레임 9 개가 필요하지만 절반에도 못 미치는 4 개가 할당되었다.

<br/>

### 비례 할당(Proportional Allocation)
비례 할당 방식은 프로세스의 크기에 비례하여 프레임을 할당하는 방식이다.

<br/>

아래의 그림에서는 A, B, C 전체 프로세스의 프레임이 6 + 3 + 9 = 18 개이고 가용 프레임이 12 개이다.

![Untitled](https://shacoding.com/wp-content/uploads/2022/06/image-56.png)

<br/>

비례 할당 방식에서는 프레임을 프로세스 A에 4 개(6 × 12 / 18), 프로세스B에 2 개(3 × 12 / 18), 프로세스 C에 6 개(9 × 12 / 18)를 할당한다.

<br/>

비례 할당은 프로세스의 크기를 고려하지 않는 고정 할당보다 좀 더 현실적인 방식이다.  
그러나 다음과 같은 2 가지 문제가 있다.  
- 프로세스가 실행 중에 필요로 하는 프레임을 유동적으로 반영하지 못한다.
    - 아무리 작은 프로세스라도 실행 중에 많은 메모리(프레임)를 필요로 하는 경우가 있다.
    - 대표적인 예로 동영상 플레이어는 프로그램 자체의 크기는 작지만, 재생되는 동영상의 크기가 크기 때문에 실행되는 동안 동영상 플레이어보다 몇 십 배 큰 메모리를 필요로 한다.
    - 이때 비례 할당 방식은 프로세스가 실행되면서 필요로 하는 프레임을 유동적으로 반영하지 못한다.
- 사용하지 않을 메모리를 처음부터 미리 확보하여 공간을 낭비한다.
    - 요구 페이지 방식에서는 아무리 큰 프로세스라도 처음부터 메모리에 모두 올리지 않는다.
    - 비례 할당 방식에서는 큰 프로세스를 실행하면서 당장 필요 없는 프레임을 미리 할당해 놓기 때문에 메모리가 낭비된다.

<br/>

## 동적 할당(Dynamic Allocation)
정적 할당 방식은 프로세스를 실행하는 초기에 프레임을 할당하기 때문에 프로세스를 실행하는 동안 메모리 요구를 반영하지 못하는 단점이 있다.  
프로세스는 실행 중에 어떨 때는 많은 프레임이 필요하기도 하고 어떨 때는 적은 프레임만으로 작동하기도 한다.  
이렇게 시시각각 변하는 요청을 수용하는 방식이 동적 할당이다.  
동적 할당 방식에는 작업집합 모델을 사용하는 방식과 페이지 부재 빈도를 사용하는 방식이 있다.

<br/>

### 작업집합 모델(Working Set Model)
작업집합 모델은 지역성 이론을 바탕으로 하며 가장 최근에 접근한 프레임이 이후에도 또 참조될 가능성이 높다는 가정에서 출발한다.  
최근 일정 시간 동안 참조된 페이지들을 집합으로 만들고, 이 집합에 있는 페이지들을 물리 메모리에 유지하여 프로세스의 실행을 돕는다.

<br/>

아래의 그림은 작업집합 모델을 나타낸 것이다.

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668854996746/qlN_W-hZI.png?auto=compress,format&format=webp)

<br/>

작업집합 모델에서 물리 메모리에 유지할 페이지의 크기를 작업집합 크기(Working Set Size)라고 하는데, 이는 작업집합에 들어갈 최대 페이지 수를 의미한다.  
위의 그림에서는 작업집합 크기를 5로 설정했다.

<br/>

작업집합에 포함되는 페이지의 범위를 작업집합 윈도우(Working Set Window, WSW)라고 한다.  
현재 시점에 최대 어느 범위까지의 페이지를 살펴볼 것인가를 결정하는 것이 작업집합 윈도우이다.  
위의 그림에서는 작업집합 윈도우를 10으로 잡고 범위를 델타로 표시했다.  
델타 동안 참조된 10 개의 페이지 중 작업집합에는 WS(t1) = {1, 7, 5, 2, 3}이 삽입되며 이 페이지들은 다음 번 윈도우에 도달할 때까지 물리 메모리에 보존된다.  
작업집합 윈도우에는 현재 시점(t1)부터 시간적으로 가까운 페이지부터 삽입된다는 것을 주의한다.

<br/>

작업집합 크기는 작업집합에 들어갈 최대 페이지 수를 말하지만 얼마나 자주 작업집합을 갱신할 것인지도 의미한다.  
작업집합 크기가 5라는 것은 페이지에 5 번 접근할 때마다 작업집합을 갱신한다는 의미이다.  
아래의 그림은 프로세스가 진행되면서 작업집합이 어떻게 변하는지 보여 준다. 

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668856969134/KHQGQ2yp_.png?auto=compress,format&format=webp)

<br/>

위의 그림은 작업집합 크기를 5로 설정했기 때문에 t1 시점 이후에 5 개 페이지에 접근하면 t2 시점에 작업집합이 갱신된다.  
또한 작업집합 윈도우를 10으로 설정했기 때문에 t2 시점에 10 개 페이지(5 5 5 7 1 3 4 4 3 2)를 살펴보고 작업집합을 WS(t2) = {2, 3, 4, 1, 7}로 갱신한다.  
원래 2 번째 작업집합 윈도우에는 {2, 3, 4, 1, 7, 5}의 6 개 페이지가 있다.  
그러나 작업집합 크기를 5로 설정했기 때문에 t2 시점에 가까운 순서대로 작업집합에 삽입되고, 가장 멀리 있는 5 번 페이지는 작업집합에 속하지 않는다.  
같은 방법으로 t2에서 5 개 페이지를 지난 t3 시점에 작업집합이 WS(t3) = {5, 3, 2, 4}로 갱신된다.

<br/>

작업집합 모델에서는 작업집합 윈도우의 크기에 따라 프로세스의 실행 성능이 달라진다.  
작업집합 윈도우를 너무 크게 잡으면 필요 없는 페이지가 메모리에 남아서 다른 프로세스에 영향을 미친다.  
반대로 윈도우를 너무 작게 잡으면 필요한 페이지가 스왑 영역으로 옮겨져서 프로세스의 성능이 떨어진다.

<br/>

### 페이지 부재 빈도(Page Fault Frequency)
작업집합 모델의 경우 충분한 페이지를 할당하지 않으면 작업집합에 있는 페이지를 물리 메모리에 유지하기가 힘들다.  
작업집합 모델에서는 어떤 프레임을 물리 메모리에 유지해야 하는지 알 수 있지만, 프로세스에 프레임을 얼마나 할당해야 하는지는 알 수 없다.  
작업집합 모델은 프로세스의 성능을 높이는 방법이지만, 스레싱 문제는 해결하지 못한다.

<br/>

프로세스가 필요로 하는 페이지의 양을 동적으로 결정하는 방법 중 페이지 부재 빈도를 이용하는 것이 있다.  
이는 페이지 부재 횟수를 기록하여 페이지 부재 비율을 계산하는 방식이다.  
페이지 부재 빈도 방식에서는 페이지 부재 비율의 상한선과 하한선을 설정한다.  
페이지 부재 비율이 상한선을 초과하면 할당한 프레임이 적다는 것을 의미하므로 프레임을 추가하여 높인다.  
반대로 페이지 부재 비율이 하한선 밑으로 내려가면 메모리가 낭비된다는 의미이므로 할당한 프레임을 회수한다.

<br/>

![Untitled](https://cdn.hashnode.com/res/hashnode/image/upload/v1668938129105/jz3SmPm7b.png?auto=compress,format&format=webp)

> 페이지 부재 비율과 페이지 할당
> 

<br/>

프로세스가 처음 시작될 때는 페이지 할당량을 예측하기 어렵다.  
페이지 부재 빈도 방식은 프로세스를 실행하면서 추가적으로 페이지를 할당하거나 회수하여 적정 페이지 할당량을 조절한다.

<br/>
<br/>

# 🗂 참고
- [책] 쉽게 배우는 운영체제
- [9-1.[os] 요구 페이징 (hashnode.dev)](https://christychoi.hashnode.dev/9-1os)
- [운영체제 - 페이지 교체 알고리즘 (yansigit.github.io)](https://yansigit.github.io/blog/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-%ED%8E%98%EC%9D%B4%EC%A7%80-%EA%B5%90%EC%B2%B4-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/)
- [9-2.[os] 페이지 교체 알고리즘 (hashnode.dev)](https://christychoi.hashnode.dev/9-2os?source=more_series_bottom_blogs)
- [9-3.[os] 스레싱과 프레임 할당 (hashnode.dev)](https://christychoi.hashnode.dev/9-3os?source=more_series_bottom_blogs)
- [OS - 가상 메모리 관리 - SHA Computing (shacoding.com)](https://shacoding.com/2022/06/11/os-%EA%B0%80%EC%83%81-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC/)
